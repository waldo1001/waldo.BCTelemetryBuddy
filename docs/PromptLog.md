# Prompt Log — User Requests

This file logs every user prompt that resulted in a significant change to the codebase. It serves as metadata for understanding the conversational flow and how to replicate the development process.

Each entry is numbered sequentially and referenced from `docs/DesignWalkthrough.md` using the format `[Prompt #N]`.

---

### Entry #1 — 2025-10-15 14:45
> "create github-copilot instructions to make sure that every change is monitored to why and how...logs this to the necessary documentation file(s)"

---

### Entry #2 — 2025-10-15 14:50
> "Add to the copilot instructions to also document/log my prompts...it's good metadata that will show me how to get to stuff. If you want, you can log the prompts in a separate file, and reference to it from the #file:DesignWalkthrough.md"

---

### Entry #3 — 2025-10-15 15:10
> "I want to be able to simply save .kql files in a workspace, and that the MCP will pick up these queries as context for resolving KQL. Remember I should have the option to 'save a query', in that case, the query should be saved as a readable .kql file in my workspace, with explanation on top of the query to have enough context about it for future reference."

---

### Entry #4 — 2025-10-15 15:20
> "I see you're not saving all my prompts. Update #file:copilot-instructions.md to make sure you do so."

---

### Entry #5 — 2025-10-15 15:25
> "in the #file:PromptLog.md, I see 'current time' - never do that, always use the exact time! Also fill all missing prompts from this conversation to the promptlog."

---

### Entry #6 — 2025-10-15 15:30
> "Read the instructions. Imagine it's finished. Answer this question: Is it going to be possible to have a workspace in VSCode, and have a specific set of KQL in there, that the Copilot will pick up as extra context?"

---

### Entry #7 — 2025-10-15 15:35
> "again, my prompt wasn't saved? always save my prompts to the #file:PromptLog.md - make sure that happens by altering #file:copilot-instructions.md"

---

### Entry #8 — 2025-10-15 15:40
> "Summarize #file:Instructions for me by giving me a short summary of the architecture."

---

### Entry #9 — 2025-10-15 15:45
> "Read the instructions. Imagine it's finished. Answer this question: will the MCP be able to get 'inspiration' from online sources as well. I know it can get inspiration from queries saved in the workspace (which it should), but additionally, I should be able to have online repos or other forms of documentation so it has most chance to create a decent query."

---

### Entry #10 — 2025-10-15 15:50
> "Add these missing capabilities to the #file:Instructions. It's important to give enough context to the agent; If there isn't enough context, the chance for compiling the right KQL query is lower."

---

### Entry #11 — 2025-10-15 15:55
> "Read the instructions. Imagine it's finished. Answer this question: will the MCP be able to get 'inspiration' from online sources as well. I know it can get inspiration from queries saved in the workspace (which it should), but additionally, I should be able to have online repos or other forms of documentation so it has most chance to create a decent query."

---

### Entry #12 — 2025-10-15 16:00
> "Summarize #file:Instructions for me by giving me a short summary of how the user experience will be of both installing all, and using it."

---

### Entry #13 — 2025-10-15 16:05
> "So that means, when starting the MCP, I will always get a dialog box, where it logs in to be able to execute queries against Azure Insights?"

---

### Entry #14 — 2025-10-15 16:10
> "Ah, so also the service-to-service authentication is foreseen in the instructions?"

---

### Entry #15 — 2025-10-15 16:15
> "If you would implement all code from #file:Instructions.md, what are the things that has to be clarified still? I don't want you to make too many decisions, so ask me now."

---

### Entry #16 — 2025-10-15 16:20
> "Here are my answers: 1. File-based cache 2. device_code (but document client_credentials well) 3. Few Shot Prompting with GitHub Copilot - filter local/online resources, LLM decides similarity, use examples to compile query 4. GitHub first, web scraping lower priority 5. No embeddings - filter by folder/filename (non-LLM), then LLM for similarity 6. Marketplace extension (free) 7. MCP starts when needed, or auto-start 8. Webview for results 9. Automatic MCP registration 10. Let queries fail with error (agent retries) - make retry count a setting 11. What is PII Sanitization? 12. Strict .kql format"

---

### Entry #17 — 2025-10-15 16:25
> "Business Central Telemetry should not contain PII information, but just in case, make it an opt-in setting, and apply it to both cached results and data sent to LLM."

---
